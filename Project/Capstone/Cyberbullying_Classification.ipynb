{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212f6e8f",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3ff2c",
   "metadata": {},
   "source": [
    "# Capstone Project: Cyberbullying Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b2698",
   "metadata": {},
   "source": [
    "Original Data: [Cyberbullying](https://www.kaggle.com/andrewmvd/cyberbullying-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a7fe3",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95fe200",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import regex as re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7353828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d8549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742bdd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==2.2.0) (3.2.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.20.1)\n",
      "Requirement already satisfied: jinja2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.6.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (4.59.0)\n",
      "Requirement already satisfied: setuptools in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (8.0.13)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.9.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.7.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy>=2.2.0->en-core-web-sm==2.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "## Prepare the stage\n",
    "#- Load spaCy\n",
    "#python3 -m spacy download en_core_web_sm\n",
    "!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab6240c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.11.0\n",
      "  latest version: 4.12.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy-model-en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a468bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages (2021.11.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9238501",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b7ec48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages (from nltk) (2021.11.2)\n",
      "Requirement already satisfied: joblib in /Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24522188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dedfd01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/anna_hj/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/anna_hj/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/anna_hj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/anna_hj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/anna_hj/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/anna_hj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anna_hj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f81bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_txt = '../Project/Capstone/cyberbullying_tweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "280ba740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4caa178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47692 entries, 0 to 47691\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   tweet_text          47692 non-null  object\n",
      " 1   cyberbullying_type  47692 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 745.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cad93ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe782d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47692"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8cbac",
   "metadata": {},
   "source": [
    "## Prepare the text\n",
    "All the text handling and preparation concerned with the changes and modifications from the raw source text to a format that will be used for the actual processing, things like:\n",
    "- handle encoding\n",
    "- handle extraneous and international charaters\n",
    "- handle simbols\n",
    "- handle metadata and embeded information\n",
    "- handle repetitions (such multiple spaces or newlines)\n",
    "\n",
    "Clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ebbd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # reduce multiple spaces and newlines to only one\n",
    "    text = re.sub(r'(\\s\\s+|\\n\\n+)', r'\\1', text)\n",
    "    # remove double quotes\n",
    "    text = re.sub(r'\"', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c5745bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  \\\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  In other words #katandandre, your food was cra...  \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "data['cleaned_text'] = data['tweet_text'].apply(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f996c",
   "metadata": {},
   "source": [
    "## Work the text\n",
    "Concern with the meaning and the substance of the content to extract actual information.\n",
    "\n",
    "Hint: Use techniques learned in previous labs. Remove StopWords, Punctuation, Lemmatize etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd802d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def clean_tweet(tweet):\n",
    " #   tweet = re.sub('http:\\S+', '', tweet)  # remove URLs\n",
    "  #  tweet = re.sub('RT|cc\\S+', '', tweet)  # remove RT and cc\n",
    "   # tweet = re.sub('#\\S+', '', tweet)  # remove hashtags\n",
    "    #tweet = re.sub('@\\S+', '', tweet)  # remove mentions\n",
    "    #tweet = re.sub('[!?.]', '', tweet)  # remove punctuations\n",
    "    #tweet = re.sub('\\s+', ' ', tweet)  # remove extra whitespace\n",
    "    #tweet = re.sub('\\s+$', '', tweet)  # remove extra whitespace\n",
    "    #text = re.sub(r'(\\s\\s+|\\n\\n+)', r'\\1', tweet)  # reduce multiple spaces and newlines to only one\n",
    "    #text = re.sub(r'\"', '', tweet)  # remove double quotes\n",
    "    #return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c797a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(tweet):\n",
    "    tweet = re.sub(r\"(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\",\"\",tweet) # remove URLs\n",
    "    return tweet\n",
    "def clean_punct_url_stopwords_single(text):\n",
    "    clean_text = \"\"\n",
    "    doc_text = nlp(text)\n",
    "    for token in doc_text:\n",
    "        if(token.like_url or token.like_email or token.is_punct or token.is_stop): #\n",
    "            continue\n",
    "        else:\n",
    "            clean_text += token.lemma_ + \" \"\n",
    "    return clean_text[:-1].strip()\n",
    "def clean_tags_mentions_single(txt):\n",
    "    patt_mention = r\"[@]\\w+\"\n",
    "    patt_tags = r\"[#]\\w+\"\n",
    "    clean_str = re.sub(patt_mention, \"\", txt)\n",
    "    clean_str = re.sub(patt_tags, \"\", clean_str)\n",
    "    clean_str = re.sub(r'[0-9]+', '', clean_str)\n",
    "    clean_str = re.sub(\"'ve\", \" have \", clean_str)\n",
    "    clean_str = re.sub(\"&amp;\", \"\", clean_str)\n",
    "    clean_str = re.sub(\"\\n\", \"\", clean_str)\n",
    "    return \" \".join(clean_str.split())\n",
    "def remove_emojis_single(txt):\n",
    "    return emoji.get_emoji_regexp().sub(u'', txt)\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "def remove_single_chars(txt):\n",
    "    return ' '.join( [w for w in txt.split() if len(w)>1] )\n",
    "def remove_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "def remove_remaining_punct(text):\n",
    "    res1 = re.sub(r\"[!@#$%^&*()_\\-=+}{\\[\\]|\\\\/<>,.?~`';:]+\", \" \", text)\n",
    "    res2 = \" \".join(res1.split())\n",
    "    return res2\n",
    "\n",
    "def clean_tweets_efficient(all_tweets):\n",
    "    start = time.time()\n",
    "    clean_tweets = list()\n",
    "    for tweet in all_tweets:\n",
    "        non_accents = remove_accents(remove_non_ascii(tweet))\n",
    "        clean_level_1 = remove_emojis_single(non_accents)\n",
    "        clean_level_2 = clean_tags_mentions_single(clean_level_1)\n",
    "        clean_level_3 = clean_punct_url_stopwords_single(clean_level_2)\n",
    "        clean_level_4 = remove_single_chars(remove_remaining_punct(clean_level_2))\n",
    "        clean_level_5 = remove_stopwords(clean_level_4)\n",
    "        clean_tweets.append(clean_level_5)\n",
    "    end = time.time()\n",
    "    print(\"Time to clean {} tweets : {} seconds\".format(len(all_tweets), end-start))\n",
    "    return clean_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5788b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(text):\n",
    "    '''\n",
    "    Use techniques learned in previous labs. Remove StopWords, Punctuation, Lemmatize etc.\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    text = ''.join(['%r'% t.lemma_ for t in doc if not (t.is_stop|t.is_punct)])\n",
    "                  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c0edf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea403c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 17s, sys: 503 ms, total: 4min 17s\n",
      "Wall time: 4min 17s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>wordkatandandrefoodcrapiliciousmkr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>aussietvwhitemkrtheblockimacelebrityautodaysun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>xochitlsuckkksclassywhoreredvelvetcupcake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>Jason_GiomehP thankheadconcernedangrydudetwitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>RudhoeEnglishisisaccountpretendkurdishaccount ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  \\\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  In other words #katandandre, your food was cra...   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...   \n",
       "\n",
       "                                               short  \n",
       "0                 wordkatandandrefoodcrapiliciousmkr  \n",
       "1  aussietvwhitemkrtheblockimacelebrityautodaysun...  \n",
       "2          xochitlsuckkksclassywhoreredvelvetcupcake  \n",
       "3   Jason_GiomehP thankheadconcernedangrydudetwitter  \n",
       "4  RudhoeEnglishisisaccountpretendkurdishaccount ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data['short'] = data['tweet_text'].apply(convert_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97bb918b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>RT @CaptCaustic: @JihadistJoe Anti mental illn...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>RT @CaptCaustic: @JihadistJoe Anti mental illn...</td>\n",
       "      <td>RTCaptCausticJihadistJoeantimentalillnessprote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29881</th>\n",
       "      <td>RT @Zombie_Bisque: @LostSailorNY @voxday @Femi...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "      <td>RT @Zombie_Bisque: @LostSailorNY @voxday @Femi...</td>\n",
       "      <td>RTzombie_bisqueLostSailorNYvoxdayFeminazi_Fron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>RT @yave145: ISIS terrorists recently killed b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>RT @yave145: ISIS terrorists recently killed b...</td>\n",
       "      <td>RTyave145isisterroristrecentlykillYPGKobanecou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28064</th>\n",
       "      <td>Alright. Time for me to get off the internet a...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "      <td>Alright. Time for me to get off the internet a...</td>\n",
       "      <td>alrighttimeinternethidebitmanagesleepgetGDClat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19233</th>\n",
       "      <td>You think Im muslim you idiot?</td>\n",
       "      <td>religion</td>\n",
       "      <td>You think Im muslim you idiot?</td>\n",
       "      <td>thinkmmuslimidiot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21724</th>\n",
       "      <td>I never liked the Catholic church but there ar...</td>\n",
       "      <td>religion</td>\n",
       "      <td>I never liked the Catholic church but there ar...</td>\n",
       "      <td>likecatholicchurchgoodChristiansharborillegala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45883</th>\n",
       "      <td>I believe the niggas at work tryn see who can ...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>I believe the niggas at work tryn see who can ...</td>\n",
       "      <td>believeniggasworktrynfuckgaggetboyfriendstupid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>ATTN ladies! If your agent is bullying you sta...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>ATTN ladies! If your agent is bullying you sta...</td>\n",
       "      <td>ATTNladyagentbullystand4urselfmoneywaygoingtos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11144</th>\n",
       "      <td>RT @lewisburgkid13 cooking and cleaning the ki...</td>\n",
       "      <td>gender</td>\n",
       "      <td>RT @lewisburgkid13 cooking and cleaning the ki...</td>\n",
       "      <td>RTlewisburgkid13cookingcleankitchenWhoDatWoman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24695</th>\n",
       "      <td>@PeerWorker @EvvyKube no. I asked if you were ...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "      <td>@PeerWorker @EvvyKube no. I asked if you were ...</td>\n",
       "      <td>PeerWorkerEvvyKubeasknuts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text   cyberbullying_type  \\\n",
       "2930   RT @CaptCaustic: @JihadistJoe Anti mental illn...    not_cyberbullying   \n",
       "29881  RT @Zombie_Bisque: @LostSailorNY @voxday @Femi...  other_cyberbullying   \n",
       "1378   RT @yave145: ISIS terrorists recently killed b...    not_cyberbullying   \n",
       "28064  Alright. Time for me to get off the internet a...  other_cyberbullying   \n",
       "19233                     You think Im muslim you idiot?             religion   \n",
       "21724  I never liked the Catholic church but there ar...             religion   \n",
       "45883  I believe the niggas at work tryn see who can ...            ethnicity   \n",
       "1485   ATTN ladies! If your agent is bullying you sta...    not_cyberbullying   \n",
       "11144  RT @lewisburgkid13 cooking and cleaning the ki...               gender   \n",
       "24695  @PeerWorker @EvvyKube no. I asked if you were ...  other_cyberbullying   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "2930   RT @CaptCaustic: @JihadistJoe Anti mental illn...   \n",
       "29881  RT @Zombie_Bisque: @LostSailorNY @voxday @Femi...   \n",
       "1378   RT @yave145: ISIS terrorists recently killed b...   \n",
       "28064  Alright. Time for me to get off the internet a...   \n",
       "19233                     You think Im muslim you idiot?   \n",
       "21724  I never liked the Catholic church but there ar...   \n",
       "45883  I believe the niggas at work tryn see who can ...   \n",
       "1485   ATTN ladies! If your agent is bullying you sta...   \n",
       "11144  RT @lewisburgkid13 cooking and cleaning the ki...   \n",
       "24695  @PeerWorker @EvvyKube no. I asked if you were ...   \n",
       "\n",
       "                                                   short  \n",
       "2930   RTCaptCausticJihadistJoeantimentalillnessprote...  \n",
       "29881  RTzombie_bisqueLostSailorNYvoxdayFeminazi_Fron...  \n",
       "1378   RTyave145isisterroristrecentlykillYPGKobanecou...  \n",
       "28064  alrighttimeinternethidebitmanagesleepgetGDClat...  \n",
       "19233                                  thinkmmuslimidiot  \n",
       "21724  likecatholicchurchgoodChristiansharborillegala...  \n",
       "45883  believeniggasworktrynfuckgaggetboyfriendstupid...  \n",
       "1485   ATTNladyagentbullystand4urselfmoneywaygoingtos...  \n",
       "11144  RTlewisburgkid13cookingcleankitchenWhoDatWoman...  \n",
       "24695                          PeerWorkerEvvyKubeasknuts  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62efe3",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3542f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "data1 = data.copy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "feature = [\"cyberbullying_type\"]\n",
    "\n",
    "\n",
    "for col in feature:\n",
    "    data1[col] = le.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cb587ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45529</th>\n",
       "      <td>@JR4_07 @TweakMonster dumb fucks....go suck ya...</td>\n",
       "      <td>1</td>\n",
       "      <td>@JR4_07 @TweakMonster dumb fucks....go suck ya...</td>\n",
       "      <td>JR4_07tweakmonsterdumbfucksuckyamammydickbitch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16090</th>\n",
       "      <td>You idiots,anti India critical of Modiji &amp;amp;...</td>\n",
       "      <td>5</td>\n",
       "      <td>You idiots,anti India critical of Modiji &amp;amp;...</td>\n",
       "      <td>idiotantiIndiacriticalModijiampShahjiforCAAamp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15455</th>\n",
       "      <td>Women are women's worst enemies but some of yo...</td>\n",
       "      <td>2</td>\n",
       "      <td>Women are women's worst enemies but some of yo...</td>\n",
       "      <td>womanwomanbadenemymakejokevictimtweetsaytarnis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41638</th>\n",
       "      <td>Stop hateing dumb ugly nigger you ain't fuckin...</td>\n",
       "      <td>1</td>\n",
       "      <td>Stop hateing dumb ugly nigger you ain't fuckin...</td>\n",
       "      <td>stophatedumbuglyniggerbefuckdumbuglyniggerfuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47374</th>\n",
       "      <td>why the fuck this dude just mention me talking...</td>\n",
       "      <td>1</td>\n",
       "      <td>why the fuck this dude just mention me talking...</td>\n",
       "      <td>fuckdudementiontalkhiNiggerdumbdryhumorignoran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>Oh. My. God. http://t.co/WHy3a8o33z http://t.c...</td>\n",
       "      <td>3</td>\n",
       "      <td>Oh. My. God. http://t.co/WHy3a8o33z http://t.c...</td>\n",
       "      <td>ohGodhttp//tco/WHy3a8o33zhttp//tco/5vsf5jroi6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34076</th>\n",
       "      <td>LOL! Someone on Twitter posted a while back th...</td>\n",
       "      <td>0</td>\n",
       "      <td>LOL! Someone on Twitter posted a while back th...</td>\n",
       "      <td>lolTwitterpostcarepichighschoolbullyvacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18960</th>\n",
       "      <td>You're an idiot nothing else. Another bhakt. T...</td>\n",
       "      <td>5</td>\n",
       "      <td>You're an idiot nothing else. Another bhakt. T...</td>\n",
       "      <td>idiotbhaktsizefamilyMuslimschildaverageHindusC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27228</th>\n",
       "      <td>@818_DodgersFan @Kanguro30 thank you! He's jus...</td>\n",
       "      <td>4</td>\n",
       "      <td>@818_DodgersFan @Kanguro30 thank you! He's jus...</td>\n",
       "      <td>818_DodgersFankanguro30thankbully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25181</th>\n",
       "      <td>@BlueTimeMachine dont bully ah ma hor!</td>\n",
       "      <td>4</td>\n",
       "      <td>@BlueTimeMachine dont bully ah ma hor!</td>\n",
       "      <td>bluetimemachinenotbullyahmahor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text  cyberbullying_type  \\\n",
       "45529  @JR4_07 @TweakMonster dumb fucks....go suck ya...                   1   \n",
       "16090  You idiots,anti India critical of Modiji &amp;...                   5   \n",
       "15455  Women are women's worst enemies but some of yo...                   2   \n",
       "41638  Stop hateing dumb ugly nigger you ain't fuckin...                   1   \n",
       "47374  why the fuck this dude just mention me talking...                   1   \n",
       "975    Oh. My. God. http://t.co/WHy3a8o33z http://t.c...                   3   \n",
       "34076  LOL! Someone on Twitter posted a while back th...                   0   \n",
       "18960  You're an idiot nothing else. Another bhakt. T...                   5   \n",
       "27228  @818_DodgersFan @Kanguro30 thank you! He's jus...                   4   \n",
       "25181             @BlueTimeMachine dont bully ah ma hor!                   4   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "45529  @JR4_07 @TweakMonster dumb fucks....go suck ya...   \n",
       "16090  You idiots,anti India critical of Modiji &amp;...   \n",
       "15455  Women are women's worst enemies but some of yo...   \n",
       "41638  Stop hateing dumb ugly nigger you ain't fuckin...   \n",
       "47374  why the fuck this dude just mention me talking...   \n",
       "975    Oh. My. God. http://t.co/WHy3a8o33z http://t.c...   \n",
       "34076  LOL! Someone on Twitter posted a while back th...   \n",
       "18960  You're an idiot nothing else. Another bhakt. T...   \n",
       "27228  @818_DodgersFan @Kanguro30 thank you! He's jus...   \n",
       "25181             @BlueTimeMachine dont bully ah ma hor!   \n",
       "\n",
       "                                                   short  \n",
       "45529  JR4_07tweakmonsterdumbfucksuckyamammydickbitch...  \n",
       "16090  idiotantiIndiacriticalModijiampShahjiforCAAamp...  \n",
       "15455  womanwomanbadenemymakejokevictimtweetsaytarnis...  \n",
       "41638  stophatedumbuglyniggerbefuckdumbuglyniggerfuck...  \n",
       "47374  fuckdudementiontalkhiNiggerdumbdryhumorignoran...  \n",
       "975        ohGodhttp//tco/WHy3a8o33zhttp//tco/5vsf5jroi6  \n",
       "34076       lolTwitterpostcarepichighschoolbullyvacation  \n",
       "18960  idiotbhaktsizefamilyMuslimschildaverageHindusC...  \n",
       "27228                  818_DodgersFankanguro30thankbully  \n",
       "25181                     bluetimemachinenotbullyahmahor  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f82b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "X = data1['tweet_text']\n",
    "y = data1['cyberbullying_type']\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "X = data1.drop(columns=['tweet_text','cyberbullying_type','cyberbullying_Yes_No','cleaned_text'])#Predictor Variable\n",
    "y = data1['cyberbullying_type'] # Target Variable\n",
    "print(\"X shape: {} \\ny shape: {}\".format(X.shape, y.shape))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d96285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "print(\"X_train shape: {} y_train shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"X_test shape : {} y_test shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7bcce9",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c147ea0",
   "metadata": {},
   "source": [
    "## Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "480620f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object\n",
    "count_vect = CountVectorizer(token_pattern = r'\\w{1,}')\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "count_vect.fit(data1['tweet_text'])\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ff478",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors as features\n",
    "- Word level\n",
    "- N-Gram level\n",
    "- Character level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4b2ea",
   "metadata": {},
   "source": [
    "### Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c8b7106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, token_pattern='\\\\w{1,}')\n",
      "CPU times: user 1.47 s, sys: 29.8 ms, total: 1.5 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word',\n",
    "                             token_pattern = r'\\w{1,}',\n",
    "                             max_features = 5000)\n",
    "print(tfidf_vect)\n",
    "\n",
    "tfidf_vect.fit(data['tweet_text'])\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf  = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14069bca",
   "metadata": {},
   "source": [
    "### N-Gram level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f4acf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, ngram_range=(2, 3), token_pattern='\\\\w{1,}')\n",
      "CPU times: user 5.78 s, sys: 188 ms, total: 5.96 s\n",
      "Wall time: 5.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                                   token_pattern = r'\\w{1,}',\n",
    "                                   ngram_range = (2, 3),\n",
    "                                   max_features = 5000)\n",
    "print(tfidf_vect_ngram)\n",
    "\n",
    "tfidf_vect_ngram.fit(data1['tweet_text'])\n",
    "X_train_tfidf_ngram = tfidf_vect_ngram.transform(X_train)\n",
    "X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda4bff",
   "metadata": {},
   "source": [
    "### Characters level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4682fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(2, 3),\n",
      "                token_pattern='\\\\w{1,}')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:546: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 217 ms, total: 10.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n",
    "                                         token_pattern = r'\\w{1,}',\n",
    "                                         ngram_range = (2, 3),\n",
    "                                         max_features = 5000)\n",
    "print(tfidf_vect_ngram_chars)\n",
    "\n",
    "tfidf_vect_ngram_chars.fit(data1['tweet_text'])\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n",
    "X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129b449",
   "metadata": {},
   "source": [
    "## Text / NLP based features\n",
    "\n",
    "Create some other features:\n",
    "\n",
    "- Char_Count = Number of Characters in Text\n",
    "- Word Count = Number of Words in Text\n",
    "- Word Density = Average Number of Char in Words\n",
    "- Punctuation Count = Number of Punctuation in Text\n",
    "- Title Word Count = Number of Words in Title\n",
    "- Uppercase Word Count = Number of Upperwords in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "293d42f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70 ms, sys: 2.41 ms, total: 72.4 ms\n",
      "Wall time: 71.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ANSWER\n",
    "data1['char_count']=data1['tweet_text'].apply(len)\n",
    "data1['word_count']=data1['tweet_text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a79dedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['word_density']=data1['char_count']/(data1['word_count']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c99377d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['punctuation_count']=data1['tweet_text'].apply(lambda x: len(''.join(_ for _ in x if _ in string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac3a2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['title_word_count']=data1['tweet_text'].apply(lambda x: len([w for w in x.split() if w.istitle()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dca224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['uppercase_word_count']=data1['tweet_text'].apply(lambda x: len([w for w in x.split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc6b81da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>short</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22987</th>\n",
       "      <td>This is the thing, you guys don’t understand, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>This is the thing, you guys don’t understand, ...</td>\n",
       "      <td>thingguyunderstandhumanconvertMuslimIslamrelig...</td>\n",
       "      <td>239</td>\n",
       "      <td>45</td>\n",
       "      <td>5.195652</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7033</th>\n",
       "      <td>RT @jbradfield: @wadhwa @freebsdgirl Personal ...</td>\n",
       "      <td>3</td>\n",
       "      <td>RT @jbradfield: @wadhwa @freebsdgirl Personal ...</td>\n",
       "      <td>RTjbradfieldwadhwafreebsdgirlpersonalagendalik...</td>\n",
       "      <td>140</td>\n",
       "      <td>20</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>I look at all these #MKR meals and wonder how ...</td>\n",
       "      <td>3</td>\n",
       "      <td>I look at all these #MKR meals and wonder how ...</td>\n",
       "      <td>lookmkrmealwonderstophamburger</td>\n",
       "      <td>86</td>\n",
       "      <td>16</td>\n",
       "      <td>5.058824</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7869</th>\n",
       "      <td>Can Kat and Andre go already? #theycantcook #MKR</td>\n",
       "      <td>3</td>\n",
       "      <td>Can Kat and Andre go already? #theycantcook #MKR</td>\n",
       "      <td>KatAndretheycantcookmkr</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32530</th>\n",
       "      <td>This woman doesn't speak for me and her 'schoo...</td>\n",
       "      <td>0</td>\n",
       "      <td>This woman doesn't speak for me and her 'schoo...</td>\n",
       "      <td>womanspeakschooluniformanalogyBSwearschoolunif...</td>\n",
       "      <td>237</td>\n",
       "      <td>43</td>\n",
       "      <td>5.386364</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25567</th>\n",
       "      <td>@slagkick I didn't acknowledge the fart I DONT...</td>\n",
       "      <td>4</td>\n",
       "      <td>@slagkick I didn't acknowledge the fart I DONT...</td>\n",
       "      <td>slagkickacknowledgefartdontknow</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43544</th>\n",
       "      <td>This is all PJs fault. Fuck him. Dumb nigger. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>This is all PJs fault. Fuck him. Dumb nigger. ...</td>\n",
       "      <td>pjfaultfuckdumbniggerIllgreygoosebottlesmashhhead</td>\n",
       "      <td>107</td>\n",
       "      <td>22</td>\n",
       "      <td>4.652174</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16148</th>\n",
       "      <td>@lbc you end Farage's contract then give airti...</td>\n",
       "      <td>5</td>\n",
       "      <td>@lbc you end Farage's contract then give airti...</td>\n",
       "      <td>lbcendFaragecontractairtimeidiotAndrewiaindale...</td>\n",
       "      <td>204</td>\n",
       "      <td>34</td>\n",
       "      <td>5.828571</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25627</th>\n",
       "      <td>So I can sympathize and understand where this ...</td>\n",
       "      <td>4</td>\n",
       "      <td>So I can sympathize and understand where this ...</td>\n",
       "      <td>sympathizeunderstandcome</td>\n",
       "      <td>65</td>\n",
       "      <td>12</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43114</th>\n",
       "      <td>A fuck about no louis dumb ugly nigger name Wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>A fuck about no louis dumb ugly nigger name Wa...</td>\n",
       "      <td>fucklouisdumbuglyniggerWaynepositivetrashsofta...</td>\n",
       "      <td>142</td>\n",
       "      <td>23</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text  cyberbullying_type  \\\n",
       "22987  This is the thing, you guys don’t understand, ...                   5   \n",
       "7033   RT @jbradfield: @wadhwa @freebsdgirl Personal ...                   3   \n",
       "708    I look at all these #MKR meals and wonder how ...                   3   \n",
       "7869    Can Kat and Andre go already? #theycantcook #MKR                   3   \n",
       "32530  This woman doesn't speak for me and her 'schoo...                   0   \n",
       "25567  @slagkick I didn't acknowledge the fart I DONT...                   4   \n",
       "43544  This is all PJs fault. Fuck him. Dumb nigger. ...                   1   \n",
       "16148  @lbc you end Farage's contract then give airti...                   5   \n",
       "25627  So I can sympathize and understand where this ...                   4   \n",
       "43114  A fuck about no louis dumb ugly nigger name Wa...                   1   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "22987  This is the thing, you guys don’t understand, ...   \n",
       "7033   RT @jbradfield: @wadhwa @freebsdgirl Personal ...   \n",
       "708    I look at all these #MKR meals and wonder how ...   \n",
       "7869    Can Kat and Andre go already? #theycantcook #MKR   \n",
       "32530  This woman doesn't speak for me and her 'schoo...   \n",
       "25567  @slagkick I didn't acknowledge the fart I DONT...   \n",
       "43544  This is all PJs fault. Fuck him. Dumb nigger. ...   \n",
       "16148  @lbc you end Farage's contract then give airti...   \n",
       "25627  So I can sympathize and understand where this ...   \n",
       "43114  A fuck about no louis dumb ugly nigger name Wa...   \n",
       "\n",
       "                                                   short  char_count  \\\n",
       "22987  thingguyunderstandhumanconvertMuslimIslamrelig...         239   \n",
       "7033   RTjbradfieldwadhwafreebsdgirlpersonalagendalik...         140   \n",
       "708                       lookmkrmealwonderstophamburger          86   \n",
       "7869                             KatAndretheycantcookmkr          48   \n",
       "32530  womanspeakschooluniformanalogyBSwearschoolunif...         237   \n",
       "25567                    slagkickacknowledgefartdontknow          51   \n",
       "43544  pjfaultfuckdumbniggerIllgreygoosebottlesmashhhead         107   \n",
       "16148  lbcendFaragecontractairtimeidiotAndrewiaindale...         204   \n",
       "25627                           sympathizeunderstandcome          65   \n",
       "43114  fucklouisdumbuglyniggerWaynepositivetrashsofta...         142   \n",
       "\n",
       "       word_count  word_density  punctuation_count  title_word_count  \\\n",
       "22987          45      5.195652                  9                 8   \n",
       "7033           20      6.666667                  5                 1   \n",
       "708            16      5.058824                  2                 1   \n",
       "7869            8      5.333333                  3                 3   \n",
       "32530          43      5.386364                  8                 3   \n",
       "25567           9      5.100000                  2                 2   \n",
       "43544          22      4.652174                  3                 4   \n",
       "16148          34      5.828571                 10                 5   \n",
       "25627          12      5.000000                  1                 2   \n",
       "43114          23      5.916667                  3                 3   \n",
       "\n",
       "       uppercase_word_count  \n",
       "22987                     0  \n",
       "7033                      1  \n",
       "708                       2  \n",
       "7869                      1  \n",
       "32530                     2  \n",
       "25567                     4  \n",
       "43544                     0  \n",
       "16148                     0  \n",
       "25627                     1  \n",
       "43114                     1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a31ebae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5dca69",
   "metadata": {},
   "source": [
    "Part of Speech in **SpaCy**\n",
    "\n",
    "    POS   DESCRIPTION               EXAMPLES\n",
    "    ----- ------------------------- ---------------------------------------------\n",
    "    ADJ   adjective                 big, old, green, incomprehensible, first\n",
    "    ADP   adposition                in, to, during\n",
    "    ADV   adverb                    very, tomorrow, down, where, there\n",
    "    AUX   auxiliary                 is, has (done), will (do), should (do)\n",
    "    CONJ  conjunction               and, or, but\n",
    "    CCONJ coordinating conjunction  and, or, but\n",
    "    DET   determiner                a, an, the\n",
    "    INTJ  interjection              psst, ouch, bravo, hello\n",
    "    NOUN  noun                      girl, cat, tree, air, beauty\n",
    "    NUM   numeral                   1, 2017, one, seventy-seven, IV, MMXIV\n",
    "    PART  particle                  's, not,\n",
    "    PRON  pronoun                   I, you, he, she, myself, themselves, somebody\n",
    "    PROPN proper noun               Mary, John, London, NATO, HBO\n",
    "    PUNCT punctuation               ., (, ), ?\n",
    "    SCONJ subordinating conjunction if, while, that\n",
    "    SYM   symbol                    $, %, §, ©, +, −, ×, ÷, =, :), 😝\n",
    "    VERB  verb                      run, runs, running, eat, ate, eating\n",
    "    X     other                     sfpksdpsxmsa\n",
    "    SPACE space\n",
    "    \n",
    "Find out number of Adjective, Adverb, Noun, Numeric, Pronoun, Proposition, Verb.\n",
    "\n",
    "    Hint:\n",
    "    1. Convert text to spacy document\n",
    "    2. Use pos_\n",
    "    3. Use Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5db3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise some columns for feature's counts\n",
    "data1['adj_count'] = 0\n",
    "data1['adv_count'] = 0\n",
    "data1['noun_count'] = 0\n",
    "data1['num_count'] = 0\n",
    "data1['pron_count'] = 0\n",
    "data1['propn_count'] = 0\n",
    "data1['verb_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75d99613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 27s, sys: 746 ms, total: 4min 28s\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ANSWER\n",
    "for i in range(data1.shape[0]):\n",
    "    doc = nlp(data1.iloc[i]['tweet_text'])\n",
    "    c = Counter([t.pos_ for t in doc])\n",
    "    data1.at[i, 'adj_count'] = c['ADJ']\n",
    "    data1.at[i, 'adv_count'] = c['ADV']\n",
    "    data1.at[i, 'noun_count'] = c['NOUN']\n",
    "    data1.at[i, 'num_count'] = c['NUM']\n",
    "    data1.at[i, 'pron_count'] = c['PRON']\n",
    "    data1.at[i, 'propn_count'] = c['PROPN']\n",
    "    data1.at[i, 'verb_count'] = c['VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83d6c919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>num_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>5.909091</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28232</th>\n",
       "      <td>84</td>\n",
       "      <td>14</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31974</th>\n",
       "      <td>248</td>\n",
       "      <td>46</td>\n",
       "      <td>5.276596</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21046</th>\n",
       "      <td>276</td>\n",
       "      <td>43</td>\n",
       "      <td>6.272727</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46257</th>\n",
       "      <td>48</td>\n",
       "      <td>9</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       char_count  word_count  word_density  punctuation_count  \\\n",
       "742            65          10      5.909091                  3   \n",
       "28232          84          14      5.600000                  3   \n",
       "31974         248          46      5.276596                  3   \n",
       "21046         276          43      6.272727                 12   \n",
       "46257          48           9      4.800000                  1   \n",
       "\n",
       "       title_word_count  uppercase_word_count  adj_count  adv_count  \\\n",
       "742                   2                     0          0          0   \n",
       "28232                 0                     1          1          1   \n",
       "31974                 2                     2          4          1   \n",
       "21046                18                     3          4          0   \n",
       "46257                 0                     5          1          0   \n",
       "\n",
       "       noun_count  num_count  pron_count  propn_count  verb_count  \n",
       "742             3          0           1            1           3  \n",
       "28232           1          0           3            3           3  \n",
       "31974           6          0           9            1          10  \n",
       "21046           5          1           5           10           8  \n",
       "46257           0          0           2            2           1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\n",
    "    'char_count', 'word_count', 'word_density',\n",
    "    'punctuation_count', 'title_word_count', \n",
    "    'uppercase_word_count','adj_count',\n",
    "    'adv_count', 'noun_count', 'num_count',\n",
    "    'pron_count', 'propn_count', 'verb_count']\n",
    "\n",
    "data1[cols].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6013bb97",
   "metadata": {},
   "source": [
    "## Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b00a2758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 23s, sys: 392 ms, total: 3min 23s\n",
      "Wall time: 3min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train a LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components = 20, learning_method = 'online', max_iter = 20)\n",
    "\n",
    "X_topics = lda_model.fit_transform(X_train_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84750ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Top Words\n",
      "----- --------------------------------------------------------------------------------\n",
      "    0 islam humanity jihad terrorist terrorists hindus hindu against human fake\n",
      "    1 after today hell enough islam yo act during found came\n",
      "    2 fucking im quran anyone own wrong lt went isis must\n",
      "    3 a rape gay jokes is and s it about the\n",
      "    4 u ur white racism word use used n colored first\n",
      "    5 gave character uh created drive main equality however lord unfortunately\n",
      "    6 believe sending worth fit upon united abusive bought absolute scum\n",
      "    7 bullying please less e de que alone tv na se\n",
      "    8 new looking cry million picture plus girlfriend 2020 forward accounts\n",
      "    9 t co http rt https w children â this is\n",
      "   10 the i to a and in school that of it\n",
      "   11 actually around bitches feminazi rt fat yourself females call abuse\n",
      "   12 you the to and are of is your in t\n",
      "   13 ð sick thinking fucked da swear buy plz republican piss\n",
      "   14 com makes http twitter gay rape https joke watch at\n",
      "   15 p looks o shame lmao fear amazing whites terrible favorite\n",
      "   16 i m a not but lol bitch rt sexist call\n",
      "   17 mkr support oh radical does better christians kat andre god\n",
      "   18 fuck dumb nigger ass you obama rt bitch tayyoung_ a\n",
      "   19 jesus judge a_man_in_black christ criminal russia skinny classic 18 record\n"
     ]
    }
   ],
   "source": [
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "print('Group Top Words')\n",
    "print('-----', '-'*80)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    top_words = ' '.join(topic_words)\n",
    "    topic_summaries.append(top_words)\n",
    "    print('  %3d %s' % (i, top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c8dab",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e009d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "    return accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a937541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the results in a dataframe\n",
    "results = pd.DataFrame(columns = ['Count Vectors',\n",
    "                                  'WordLevel TF-IDF',\n",
    "                                  'N-Gram Vectors',\n",
    "                                  'CharLevel Vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0930b9",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e56d3de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors    : 0.7645\n",
      "\n",
      "CPU times: user 18.7 ms, sys: 5.35 ms, total: 24 ms\n",
      "Wall time: 24.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy1 = train_model(MultinomialNB(), X_train_count, y_train, X_test_count)\n",
    "print('NB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "226721bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF : 0.7807\n",
      "\n",
      "CPU times: user 11.6 ms, sys: 4.81 ms, total: 16.4 ms\n",
      "Wall time: 15.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('NB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8ff187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors   : 0.6665\n",
      "\n",
      "CPU times: user 9.47 ms, sys: 2.71 ms, total: 12.2 ms\n",
      "Wall time: 10.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('NB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50df033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, CharLevel Vectors: 0.7493\n",
      "\n",
      "CPU times: user 50.2 ms, sys: 33.9 ms, total: 84.1 ms\n",
      "Wall time: 84.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('NB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f43a5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Naïve Bayes'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308db8c2",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e229dea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors    : 0.8229\n",
      "\n",
      "CPU times: user 1min 8s, sys: 13.5 s, total: 1min 21s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy1 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 350), X_train_count, y_train, X_test_count)\n",
    "print('LR, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06e55598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF : 0.8355\n",
      "\n",
      "CPU times: user 13.2 s, sys: 2.4 s, total: 15.6 s\n",
      "Wall time: 1.99 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('LR, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "838237bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors   : 0.7322\n",
      "\n",
      "CPU times: user 10.4 s, sys: 1.99 s, total: 12.3 s\n",
      "Wall time: 1.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('LR, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe5b536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, CharLevel Vectors: 0.8388\n",
      "\n",
      "CPU times: user 1min 3s, sys: 11.3 s, total: 1min 14s\n",
      "Wall time: 9.76 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna_hj/opt/anaconda3/envs/IOD-deep-learning/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('LR, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b7de964",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Logistic Regression'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6be07",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d196762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors    : 0.8131\n",
      "\n",
      "CPU times: user 3.19 s, sys: 241 ms, total: 3.43 s\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Count Vectors\n",
    "accuracy1 = train_model(LinearSVC(), X_train_count, y_train, X_test_count)\n",
    "print('SVM, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9375576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF : 0.8327\n",
      "\n",
      "CPU times: user 336 ms, sys: 3.9 ms, total: 340 ms\n",
      "Wall time: 339 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LinearSVC(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('SVM, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92710b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors   : 0.7304\n",
      "\n",
      "CPU times: user 295 ms, sys: 2.68 ms, total: 297 ms\n",
      "Wall time: 296 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LinearSVC(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('SVM, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a5a2fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, CharLevel Vectors: 0.8349\n",
      "\n",
      "CPU times: user 2.16 s, sys: 37.1 ms, total: 2.19 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LinearSVC(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('SVM, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42772525",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Support Vector Machine'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef48012",
   "metadata": {},
   "source": [
    "## Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5452c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors    : 0.8056\n",
      "\n",
      "CPU times: user 3min 20s, sys: 400 ms, total: 3min 21s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Count Vectors\n",
    "accuracy1 = train_model(RandomForestClassifier(n_estimators = 100), X_train_count, y_train, X_test_count)\n",
    "print('RF, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f67156c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF : 0.8093\n",
      "\n",
      "CPU times: user 54.2 s, sys: 129 ms, total: 54.4 s\n",
      "Wall time: 54.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('RF, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6de52ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, N-Gram Vectors   : 0.7116\n",
      "\n",
      "CPU times: user 55.1 s, sys: 113 ms, total: 55.2 s\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('RF, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9245781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, CharLevel Vectors: 0.8141\n",
      "\n",
      "CPU times: user 3min 26s, sys: 461 ms, total: 3min 27s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('RF, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "477bfbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Random Forest'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9ed8d",
   "metadata": {},
   "source": [
    "## Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "925af900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, Count Vectors    : 0.8310\n",
      "\n",
      "CPU times: user 10min 37s, sys: 969 ms, total: 10min 38s\n",
      "Wall time: 10min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Count Vectors\n",
    "accuracy1 = train_model(GradientBoostingClassifier(), X_train_count, y_train, X_test_count)\n",
    "print('GB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abd696f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, WordLevel TF-IDF : 0.8353\n",
      "\n",
      "CPU times: user 1min 49s, sys: 130 ms, total: 1min 49s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(GradientBoostingClassifier(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('GB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e51031b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, N-Gram Vectors   : 0.6912\n",
      "\n",
      "CPU times: user 48.8 s, sys: 69.7 ms, total: 48.9 s\n",
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('GB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9703b6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, CharLevel Vectors: 0.8434\n",
      "\n",
      "CPU times: user 19min 36s, sys: 1.78 s, total: 19min 38s\n",
      "Wall time: 19min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('GB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc9e5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc['Gradient Boosting'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e9db91c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count Vectors</th>\n",
       "      <th>WordLevel TF-IDF</th>\n",
       "      <th>N-Gram Vectors</th>\n",
       "      <th>CharLevel Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naïve Bayes</th>\n",
       "      <td>0.764546</td>\n",
       "      <td>0.780690</td>\n",
       "      <td>0.666527</td>\n",
       "      <td>0.749345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.822937</td>\n",
       "      <td>0.835517</td>\n",
       "      <td>0.732152</td>\n",
       "      <td>0.838767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.813083</td>\n",
       "      <td>0.832687</td>\n",
       "      <td>0.730370</td>\n",
       "      <td>0.834888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.805640</td>\n",
       "      <td>0.809309</td>\n",
       "      <td>0.711605</td>\n",
       "      <td>0.814131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.831010</td>\n",
       "      <td>0.835308</td>\n",
       "      <td>0.691163</td>\n",
       "      <td>0.843380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Count Vectors  WordLevel TF-IDF  N-Gram Vectors  \\\n",
       "Naïve Bayes                  0.764546          0.780690        0.666527   \n",
       "Logistic Regression          0.822937          0.835517        0.732152   \n",
       "Support Vector Machine       0.813083          0.832687        0.730370   \n",
       "Random Forest                0.805640          0.809309        0.711605   \n",
       "Gradient Boosting            0.831010          0.835308        0.691163   \n",
       "\n",
       "                        CharLevel Vectors  \n",
       "Naïve Bayes                      0.749345  \n",
       "Logistic Regression              0.838767  \n",
       "Support Vector Machine           0.834888  \n",
       "Random Forest                    0.814131  \n",
       "Gradient Boosting                0.843380  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccbca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba2e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ea783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to show results and charts\n",
    "def show_summary_report(actual, prediction):\n",
    "\n",
    "    if isinstance(actual, pd.Series):\n",
    "        actual = actual.values\n",
    "    if actual.dtype.name == 'object':\n",
    "        actual = actual.astype(int)\n",
    "    if prediction.dtype.name == 'object':\n",
    "        prediction = prediction.astype(int)\n",
    "\n",
    "    accuracy_ = accuracy_score(actual, prediction)\n",
    "    precision_ = precision_score(actual, prediction)\n",
    "    recall_ = recall_score(actual, prediction)\n",
    "    roc_auc_ = roc_auc_score(actual, prediction)\n",
    "\n",
    "    print('Accuracy : %.4f [TP / N] Proportion of predicted labels that match the true labels. Best: 1, Worst: 0' % accuracy_)\n",
    "    print('Precision: %.4f [TP / (TP + FP)] Not to label a negative sample as positive.        Best: 1, Worst: 0' % precision_)\n",
    "    print('Recall   : %.4f [TP / (TP + FN)] Find all the positive samples.                     Best: 1, Worst: 0' % recall_)\n",
    "    print('ROC AUC  : %.4f                                                                     Best: 1, Worst: < 0.5' % roc_auc_)\n",
    "    print('-' * 107)\n",
    "    print('TP: True Positives, FP: False Positives, TN: True Negatives, FN: False Negatives, N: Number of samples')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    mat = confusion_matrix(actual, prediction)\n",
    "\n",
    "    # Precision/Recall\n",
    "    precision, recall, _ = precision_recall_curve(actual, prediction)\n",
    "    average_precision = average_precision_score(actual, prediction)\n",
    "    \n",
    "    # Compute ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(actual, prediction)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (18, 6))\n",
    "    fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(mat.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Blues', ax = ax[0])\n",
    "\n",
    "    ax[0].set_title('Confusion Matrix')\n",
    "    ax[0].set_xlabel('True label')\n",
    "    ax[0].set_ylabel('Predicted label')\n",
    "    \n",
    "    # Precision/Recall\n",
    "    step_kwargs = {'step': 'post'}\n",
    "    ax[1].step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n",
    "    ax[1].fill_between(recall, precision, alpha = 0.2, color = 'b', **step_kwargs)\n",
    "    ax[1].set_ylim([0.0, 1.0])\n",
    "    ax[1].set_xlim([0.0, 1.0])\n",
    "    ax[1].set_xlabel('Recall')\n",
    "    ax[1].set_ylabel('Precision')\n",
    "    ax[1].set_title('2-class Precision-Recall curve')\n",
    "\n",
    "    # ROC\n",
    "    ax[2].plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    ax[2].plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--')\n",
    "    ax[2].set_xlim([0.0, 1.0])\n",
    "    ax[2].set_ylim([0.0, 1.0])\n",
    "    ax[2].set_xlabel('False Positive Rate')\n",
    "    ax[2].set_ylabel('True Positive Rate')\n",
    "    ax[2].set_title('Receiver Operating Characteristic')\n",
    "    ax[2].legend(loc = 'lower right')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return (accuracy_, precision_, recall_, roc_auc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2303f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "X = data1['short']\n",
    "y = data1['cyberbullying_type']\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8527a0",
   "metadata": {},
   "source": [
    "## Repeat using TF-IDF\n",
    "TF-IDF = Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of word counts from the text\n",
    "# use TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "# do the actual counting\n",
    "A = tfidf.fit_transform(X_train, y_train)\n",
    "\n",
    "# train the classifier with the training data\n",
    "classifier.fit(A.toarray(), y_train)\n",
    "\n",
    "# do the transformation for the test data\n",
    "# NOTE: use `transform()` instead of `fit_transform()`\n",
    "B = tfidf.transform(X_test)\n",
    "\n",
    "# make predictions based on the test data\n",
    "predictions = classifier.predict(B.toarray())\n",
    "\n",
    "# check the accuracy\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_summary_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91775050",
   "metadata": {},
   "source": [
    "## Use Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d734e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of word counts from the text\n",
    "counts = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the actual counting\n",
    "A = counts.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a classifier using SVC\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='rbf', probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b70afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the classifier with the training data\n",
    "classifier.fit(A.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the transformation for the test data\n",
    "# NOTE: use `transform()` instead of `fit_transform()`\n",
    "B = counts.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e770b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475afa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdace34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions based on the test data\n",
    "predictions = classifier.predict(B.toarray())\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9541ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(A.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(B.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1929d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the accuracy\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526ae09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1511f481",
   "metadata": {},
   "source": [
    "## Repeating it all for comparision\n",
    "Repeating the whole lot in one big block\n",
    "\n",
    "Find 'Accuracy', 'Precision', 'Recall', 'ROC_AUC' using CountVectorizer and TfidfVectorizer and keep the result in a dataframe.\n",
    "(9.6 exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff42ee",
   "metadata": {},
   "source": [
    "## Model building\n",
    "### Here, we will use Bag of Words and TF-IDF method to generate matrix from text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d7332",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(max_features=5000)\n",
    "train_mat_bow = bow.fit_transform(X_train['cleaned_stopwords']).toarray()\n",
    "test_mat_bow = bow.transform(X_test['cleaned_stopwords']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6dc48",
   "metadata": {},
   "source": [
    "#### Since it is multi class classification problem, will use\n",
    "- Naive Bayes\n",
    "- Decision Tree\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_bow = MultinomialNB()\n",
    "dt_bow = DecisionTreeClassifier(random_state=42)\n",
    "rf_bow = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clfs = {\n",
    "    \"Naive Bayes\": nb_bow,\n",
    "    \"Decision Tree\": dt_bow,\n",
    "    \"Random Forest\": rf_bow\n",
    "}\n",
    "\n",
    "def fit_model(clf,x_train,y_train,x_test, y_test):\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for name,clf in clfs.items():\n",
    "    curr_acc = fit_model(clf,train_mat_bow,y_train,test_mat_bow,y_test)\n",
    "    accuracy.append(curr_acc)\n",
    "    \n",
    "models_df = pd.DataFrame({\"Models\":clfs.keys(),\"Accuracy Scores\":accuracy}).sort_values('Accuracy Scores',ascending=False)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56b02b",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc7d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "train_m_tfidf = tfidf.fit_transform(X_train['cleaned_stopwords']).toarray()\n",
    "test_m_tfidf = tfidf.transform(X_test['cleaned_stopwords']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tf = MultinomialNB()\n",
    "dt_tf = DecisionTreeClassifier(random_state=42)\n",
    "rf_tf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clfs = {\n",
    "    \"Naive Bayes\": nb_tf,\n",
    "    \"Decision Tree\": dt_tf,\n",
    "    \"Random Forest\": rf_tf,\n",
    "}\n",
    "\n",
    "def fit_model(clf,x_train,y_train,x_test, y_test):\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for name,clf in clfs.items():\n",
    "    curr_acc = fit_model(clf,train_m_tfidf,y_train,test_m_tfidf,y_te)\n",
    "    accuracy.append(curr_acc)\n",
    "    \n",
    "models_df = pd.DataFrame({\"Models\":clfs.keys(),\"Accuracy Scores\":accuracy}).sort_values('Accuracy Scores',ascending=False)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa82b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame({\"Models\":clfs.keys(),\"Accuracy Scores\":accuracys}).sort_values('Accuracy Scores',ascending=False)\n",
    "models_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
