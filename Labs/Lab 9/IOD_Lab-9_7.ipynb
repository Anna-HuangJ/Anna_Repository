{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nji1a9ULLtCA"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnsX1AWKLtCE"
   },
   "source": [
    "# Lab 9.7: Text Classification\n",
    "INSTRUCTIONS:\n",
    "- Run the cells\n",
    "- Observe and understand the results\n",
    "- Answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pm8PttyLtCI"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:33.182995Z",
     "start_time": "2019-06-17T01:38:30.045388Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EUANiH6zLtCK"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58bUNQA0LtCV"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqU7d_qcLtCX"
   },
   "source": [
    "Sample:\n",
    "\n",
    "    __label__2 Stuning even for the non-gamer: This sound ...\n",
    "    __label__2 The best soundtrack ever to anything.: I'm ...\n",
    "    __label__2 Amazing!: This soundtrack is my favorite m ...\n",
    "    __label__2 Excellent Soundtrack: I truly like this so ...\n",
    "    __label__2 Remember, Pull Your Jaw Off The Floor Afte ...\n",
    "    __label__2 an absolute masterpiece: I am quite sure a ...\n",
    "    __label__1 Buyer beware: This is a self-published boo ...\n",
    "    . . .\n",
    "    \n",
    "There are only two **labels**:\n",
    "- `__label__1`\n",
    "- `__label__2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:42.024845Z",
     "start_time": "2019-06-17T01:38:41.896098Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rwWFJprZLtCZ"
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "\n",
    "trainDF = pd.read_fwf(\n",
    "    filepath_or_buffer = '../DATA/corpus.txt',\n",
    "    colspecs = [(9, 10),   # label: get only the numbers 1 or 2\n",
    "                (11, 9000) # text: makes the it big enought to get to the end of the line\n",
    "               ], \n",
    "    header = 0,\n",
    "    names = ['label', 'text'],\n",
    "    lineterminator = '\\n'\n",
    ")\n",
    "\n",
    "# convert label from [1, 2] to [0, 1]\n",
    "trainDF['label'] = trainDF['label'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mILVIHomLtCf"
   },
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:39:24.213192Z",
     "start_time": "2019-06-17T01:39:24.209202Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "G9_8RbOeLtCh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   9999 non-null   int64 \n",
      " 1   text    9999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n",
      "None\n",
      "      label                                               text\n",
      "3611      0  What are these guys?: Oh man. I don't even kno...\n",
      "7851      1  A Soul Star he is: this album is so old school...\n",
      "7188      1  The Hearth and the Salamander: A future in whi...\n",
      "2398      0  Dean Koontz's Frankenstein: I imagine this is ...\n",
      "9752      1  An Outstanding Read: An outstanding read about...\n",
      "2437      1  Love it!: I am very happy with the weight and ...\n",
      "3254      0  The original recording probably sounded good, ...\n",
      "1104      0  Don't listen to the others!: Absolutely awful....\n",
      "7255      0  A Time Waste: Simply unreadable. A nonsense bo...\n",
      "1258      1  La reencarnación vista por un científico: El p...\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "print(trainDF.info())\n",
    "print(trainDF.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YmYgG2pLtCu"
   },
   "source": [
    "## Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:39:40.103737Z",
     "start_time": "2019-06-17T01:39:40.100739Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "j5vErjWFLtCy"
   },
   "outputs": [],
   "source": [
    "## ANSWER\n",
    "## split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "trainDF['text'],\n",
    "trainDF['label'],\n",
    "test_size= 0.2,\n",
    "random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nUp6oDOLtC1"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKd9yTnyLtC2"
   },
   "source": [
    "## Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:32.674674Z",
     "start_time": "2019-06-17T01:40:31.098889Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DU2RqqDjLtC3"
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object\n",
    "count_vect = CountVectorizer(token_pattern = r'\\w{1,}')\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJs6al0ILtC5"
   },
   "source": [
    "## TF-IDF Vectors as features\n",
    "- Word level\n",
    "- N-Gram level\n",
    "- Character level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:36.088730Z",
     "start_time": "2019-06-17T01:40:34.519925Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "myjfdfP_LtC6",
    "outputId": "8bc4d529-1f66-4836-acd1-07633c29fd02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, token_pattern='\\\\w{1,}')\n",
      "CPU times: user 900 ms, sys: 10.5 ms, total: 910 ms\n",
      "Wall time: 910 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word',\n",
    "                             token_pattern = r'\\w{1,}',\n",
    "                             max_features = 5000)\n",
    "print(tfidf_vect)\n",
    "\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf  = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:57.505221Z",
     "start_time": "2019-06-17T01:40:49.387393Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-h16dUaVLtC_",
    "outputId": "3be1f8f5-670e-4249-8522-50509c8898a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, ngram_range=(2, 3), token_pattern='\\\\w{1,}')\n",
      "CPU times: user 3.76 s, sys: 83.5 ms, total: 3.85 s\n",
      "Wall time: 3.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                                   token_pattern = r'\\w{1,}',\n",
    "                                   ngram_range = (2, 3),\n",
    "                                   max_features = 5000)\n",
    "print(tfidf_vect_ngram)\n",
    "\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "X_train_tfidf_ngram = tfidf_vect_ngram.transform(X_train)\n",
    "X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:41:10.209071Z",
     "start_time": "2019-06-17T01:40:59.211484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Y7rmIt49LtDC",
    "outputId": "8f600e7c-b4df-4d89-bfb9-96c8436aa5ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(2, 3),\n",
      "                token_pattern='\\\\w{1,}')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna_hj/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.91 s, sys: 104 ms, total: 7.01 s\n",
      "Wall time: 7.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n",
    "                                         token_pattern = r'\\w{1,}',\n",
    "                                         ngram_range = (2, 3),\n",
    "                                         max_features = 5000)\n",
    "print(tfidf_vect_ngram_chars)\n",
    "\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n",
    "X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Pck1cuvLtDH"
   },
   "source": [
    "## Text / NLP based features\n",
    "\n",
    "Create some other features:\n",
    "\n",
    "- Char_Count = Number of Characters in Text\n",
    "- Word Count = Number of Words in Text\n",
    "- Word Density = Average Number of Char in Words\n",
    "- Punctuation Count = Number of Punctuation in Text\n",
    "- Title Word Count = Number of Words in Title\n",
    "- Uppercase Word Count = Number of Upperwords in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:43:52.813132Z",
     "start_time": "2019-06-17T01:43:52.806150Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4jebGm1gLtDH",
    "outputId": "124d1ac4-9b64-414d-e973-9ded78662e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.6 ms, sys: 982 µs, total: 40.6 ms\n",
      "Wall time: 39.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ANSWER\n",
    "trainDF['char_count']=trainDF['text'].apply(len)\n",
    "trainDF['word_count']=trainDF['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['word_density']=trainDF['char_count']/(trainDF['word_count']+1)\n",
    "trainDF['punctuation_count']=trainDF['text'].apply(lambda x: len(''.join(_ for _ in x if _ in string.punctuation)))\n",
    "trainDF['title_word_count']=trainDF['text'].apply(lambda x: len([w for w in x.split() if w.istitle()]))\n",
    "trainDF['uppercase_word_count']=trainDF['text'].apply(lambda x: len([w for w in x.split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8668</th>\n",
       "      <td>0</td>\n",
       "      <td>buyer beware...this product and the company ar...</td>\n",
       "      <td>859</td>\n",
       "      <td>152</td>\n",
       "      <td>5.614379</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>1</td>\n",
       "      <td>Not used yet: These are spares for the wristro...</td>\n",
       "      <td>119</td>\n",
       "      <td>23</td>\n",
       "      <td>4.958333</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6313</th>\n",
       "      <td>1</td>\n",
       "      <td>Very Nearly Perfect: This is one of few films ...</td>\n",
       "      <td>445</td>\n",
       "      <td>69</td>\n",
       "      <td>6.357143</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7577</th>\n",
       "      <td>0</td>\n",
       "      <td>SQUEKY SPRINGS: In my opinion this is a strict...</td>\n",
       "      <td>188</td>\n",
       "      <td>31</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>1</td>\n",
       "      <td>Simple Installation: I received and installed ...</td>\n",
       "      <td>620</td>\n",
       "      <td>113</td>\n",
       "      <td>5.438596</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm happy with mine.: I've had no problems wit...</td>\n",
       "      <td>570</td>\n",
       "      <td>105</td>\n",
       "      <td>5.377358</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4644</th>\n",
       "      <td>1</td>\n",
       "      <td>Forget The Hype! This is a Great Record!: Forg...</td>\n",
       "      <td>904</td>\n",
       "      <td>164</td>\n",
       "      <td>5.478788</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>0</td>\n",
       "      <td>An Album Minimalist in Genre AND Enjoyment: As...</td>\n",
       "      <td>659</td>\n",
       "      <td>112</td>\n",
       "      <td>5.831858</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>1</td>\n",
       "      <td>The Best Live Harmony You can Get!: Absolutely...</td>\n",
       "      <td>195</td>\n",
       "      <td>34</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8763</th>\n",
       "      <td>0</td>\n",
       "      <td>Nothing new here, just a Curve [copy] band.: S...</td>\n",
       "      <td>132</td>\n",
       "      <td>23</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  char_count  \\\n",
       "8668      0  buyer beware...this product and the company ar...         859   \n",
       "7579      1  Not used yet: These are spares for the wristro...         119   \n",
       "6313      1  Very Nearly Perfect: This is one of few films ...         445   \n",
       "7577      0  SQUEKY SPRINGS: In my opinion this is a strict...         188   \n",
       "9541      1  Simple Installation: I received and installed ...         620   \n",
       "3475      1  I'm happy with mine.: I've had no problems wit...         570   \n",
       "4644      1  Forget The Hype! This is a Great Record!: Forg...         904   \n",
       "1057      0  An Album Minimalist in Genre AND Enjoyment: As...         659   \n",
       "1602      1  The Best Live Harmony You can Get!: Absolutely...         195   \n",
       "8763      0  Nothing new here, just a Curve [copy] band.: S...         132   \n",
       "\n",
       "      word_count  word_density  punctuation_count  title_word_count  \\\n",
       "8668         152      5.614379                 38                 2   \n",
       "7579          23      4.958333                  2                 3   \n",
       "6313          69      6.357143                 19                 9   \n",
       "7577          31      5.875000                  4                 2   \n",
       "9541         113      5.438596                  8                14   \n",
       "3475         105      5.377358                 26                 5   \n",
       "4644         164      5.478788                 37                33   \n",
       "1057         112      5.831858                 18                11   \n",
       "1602          34      5.571429                  4                10   \n",
       "8763          23      5.500000                  8                 5   \n",
       "\n",
       "      uppercase_word_count  \n",
       "8668                    18  \n",
       "7579                     1  \n",
       "6313                     0  \n",
       "7577                     3  \n",
       "9541                     7  \n",
       "3475                     7  \n",
       "4644                    11  \n",
       "1057                     3  \n",
       "1602                     0  \n",
       "8763                     0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:44:03.442730Z",
     "start_time": "2019-06-17T01:44:02.298791Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Z-l2iZcLLtDO",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## load spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-9d0G59LtDR"
   },
   "source": [
    "Part of Speech in **SpaCy**\n",
    "\n",
    "    POS   DESCRIPTION               EXAMPLES\n",
    "    ----- ------------------------- ---------------------------------------------\n",
    "    ADJ   adjective                 big, old, green, incomprehensible, first\n",
    "    ADP   adposition                in, to, during\n",
    "    ADV   adverb                    very, tomorrow, down, where, there\n",
    "    AUX   auxiliary                 is, has (done), will (do), should (do)\n",
    "    CONJ  conjunction               and, or, but\n",
    "    CCONJ coordinating conjunction  and, or, but\n",
    "    DET   determiner                a, an, the\n",
    "    INTJ  interjection              psst, ouch, bravo, hello\n",
    "    NOUN  noun                      girl, cat, tree, air, beauty\n",
    "    NUM   numeral                   1, 2017, one, seventy-seven, IV, MMXIV\n",
    "    PART  particle                  's, not,\n",
    "    PRON  pronoun                   I, you, he, she, myself, themselves, somebody\n",
    "    PROPN proper noun               Mary, John, London, NATO, HBO\n",
    "    PUNCT punctuation               ., (, ), ?\n",
    "    SCONJ subordinating conjunction if, while, that\n",
    "    SYM   symbol                    $, %, §, ©, +, −, ×, ÷, =, :), 😝\n",
    "    VERB  verb                      run, runs, running, eat, ate, eating\n",
    "    X     other                     sfpksdpsxmsa\n",
    "    SPACE space\n",
    "    \n",
    "Find out number of Adjective, Adverb, Noun, Numeric, Pronoun, Proposition, Verb.\n",
    "\n",
    "    Hint:\n",
    "    1. Convert text to spacy document\n",
    "    2. Use pos_\n",
    "    3. Use Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:50:15.900377Z",
     "start_time": "2019-06-17T01:50:15.889406Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NcxmvIOGLtDS"
   },
   "outputs": [],
   "source": [
    "# Initialise some columns for feature's counts\n",
    "trainDF['adj_count'] = 0\n",
    "trainDF['adv_count'] = 0\n",
    "trainDF['noun_count'] = 0\n",
    "trainDF['num_count'] = 0\n",
    "trainDF['pron_count'] = 0\n",
    "trainDF['propn_count'] = 0\n",
    "trainDF['verb_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:52:39.611809Z",
     "start_time": "2019-06-17T01:52:39.608818Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1KfFtu1HcPwA"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# ANSWER\n",
    "for i in range(trainDF.shape[0]):\n",
    "    doc = nlp(trainDF.iloc[i]['text'])\n",
    "    c = Counter([t.pos_ for t in doc])\n",
    "    trainDF.at[i, 'adj_count'] = c['ADJ']\n",
    "    trainDF.at[i, 'adv_count'] = c['ADV']\n",
    "    trainDF.at[i, 'noun_count'] = c['NOUN']\n",
    "    trainDF.at[i, 'num_count'] = c['NUM']\n",
    "    trainDF.at[i, 'pron_count'] = c['PRON']\n",
    "    trainDF.at[i, 'propn_count'] = c['PROPN']\n",
    "    trainDF.at[i, 'verb_count'] = c['VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:59:42.424828Z",
     "start_time": "2019-06-17T01:59:42.390920Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DW1_LKP2LtDX",
    "outputId": "7a5eb5fd-cae1-4e76-f95f-e0fe9f1780d8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'char_count', 'word_count', 'word_density',\n",
    "    'punctuation_count', 'title_word_count', \n",
    "    'uppercase_word_count','adj_count',\n",
    "    'adv_count', 'noun_count', 'num_count',\n",
    "    'pron_count', 'propn_count', 'verb_count']\n",
    "\n",
    "trainDF[cols].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQCAUFWYLtDb"
   },
   "source": [
    "## Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:27:09.442903Z",
     "start_time": "2019-06-17T02:24:45.531924Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wg2mAlkRLtDb",
    "outputId": "323bfce4-9263-403a-9214-e9e206d5755f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train a LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components = 20, learning_method = 'online', max_iter = 20)\n",
    "\n",
    "X_topics = lda_model.fit_transform(X_train_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:28:11.804475Z",
     "start_time": "2019-06-17T02:28:10.978502Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_8dIDyHjLtDf",
    "outputId": "ea0614e2-66f1-4ddd-bff0-142cfd4fd78a"
   },
   "outputs": [],
   "source": [
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "print('Group Top Words')\n",
    "print('-----', '-'*80)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    top_words = ' '.join(topic_words)\n",
    "    topic_summaries.append(top_words)\n",
    "    print('  %3d %s' % (i, top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TtfnK1jeLtDl"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:12.273365Z",
     "start_time": "2019-06-17T02:34:12.263393Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uwVaWSyTLtDm"
   },
   "outputs": [],
   "source": [
    "## helper function\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "    return accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:14.900001Z",
     "start_time": "2019-06-17T02:34:14.894016Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "f_onpqUkLtDo"
   },
   "outputs": [],
   "source": [
    "# Keep the results in a dataframe\n",
    "results = pd.DataFrame(columns = ['Count Vectors',\n",
    "                                  'WordLevel TF-IDF',\n",
    "                                  'N-Gram Vectors',\n",
    "                                  'CharLevel Vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXwLriDpLtDq"
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:34.147043Z",
     "start_time": "2019-06-17T02:34:34.123096Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZcU6IKyNLtDs",
    "outputId": "d2defcfb-2046-47e1-b3b6-08d6edca94f8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy1 = train_model(MultinomialNB(), X_train_count, y_train, X_test_count)\n",
    "print('NB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:36.399812Z",
     "start_time": "2019-06-17T02:34:36.381861Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zqEG_ByTLtDv",
    "outputId": "0ba74f49-a71c-4d59-f57b-f39f546241ce"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('NB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:39.076000Z",
     "start_time": "2019-06-17T02:34:39.059047Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uKEEPNi8LtDy",
    "outputId": "c00cf532-c914-4670-cde5-69bf54bf7201"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('NB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:42.057019Z",
     "start_time": "2019-06-17T02:34:42.009151Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "M9aIibkBLtD0",
    "outputId": "1f52c1f4-150d-4195-f927-cb66ec41baba"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('NB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:46.265712Z",
     "start_time": "2019-06-17T02:34:46.258734Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kkrodUzCLtD3"
   },
   "outputs": [],
   "source": [
    "results.loc['Naïve Bayes'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2oNfajULtD4"
   },
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:50.841687Z",
     "start_time": "2019-06-17T02:34:48.637032Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OFBhhPZ6LtD4",
    "outputId": "cb5a2b19-dfc1-4b11-e698-0488ba4eae53"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy1 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 350), X_train_count, y_train, X_test_count)\n",
    "print('LR, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.310433Z",
     "start_time": "2019-06-17T02:34:51.214690Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "C89hRhDiLtD6",
    "outputId": "023669b1-788f-4a1b-d282-99464e26312c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('LR, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.749258Z",
     "start_time": "2019-06-17T02:34:51.683435Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MvhV1jC6LtD9",
    "outputId": "9bef9b1d-2dc3-4300-c4ff-831957aadee4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('LR, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:52.635899Z",
     "start_time": "2019-06-17T02:34:52.175122Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XPjIxmtKLtEA",
    "outputId": "58f4b0e9-e786-45a1-830e-5945129792d4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('LR, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:53.029844Z",
     "start_time": "2019-06-17T02:34:53.018872Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZFK_LWTcLtED"
   },
   "outputs": [],
   "source": [
    "results.loc['Logistic Regression'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1wYto68LtEE"
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.237613Z",
     "start_time": "2019-06-17T02:34:53.406835Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yYGz8he5LtEE",
    "outputId": "546b1ea8-27c9-45bb-fd91-da08244d6796"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Count Vectors\n",
    "accuracy1 = train_model(LinearSVC(), X_train_count, y_train, X_test_count)\n",
    "print('SVM, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.743263Z",
     "start_time": "2019-06-17T02:34:54.606629Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wMt26K0cLtEG",
    "outputId": "406cf62d-c09d-4f9b-c298-eabce0b69238"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LinearSVC(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('SVM, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:55.220003Z",
     "start_time": "2019-06-17T02:34:55.119256Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eFt6Y1VvLtEI",
    "outputId": "628299db-8bca-4698-c64e-5b291bd248e1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LinearSVC(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('SVM, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.139528Z",
     "start_time": "2019-06-17T02:34:55.585010Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iqhsS579LtEL",
    "outputId": "7cc3d723-9858-43a8-dcdf-37fcbef9456e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LinearSVC(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('SVM, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.501558Z",
     "start_time": "2019-06-17T02:34:56.492592Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "go0bcKeILtEN"
   },
   "outputs": [],
   "source": [
    "results.loc['Support Vector Machine'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLGxWK0yLtEO"
   },
   "source": [
    "## Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:20.807157Z",
     "start_time": "2019-06-17T02:34:56.823697Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HR8aOytWLtEO",
    "outputId": "961d5bca-c1fe-46be-ba8c-2f6325d85901"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Count Vectors\n",
    "accuracy1 = train_model(RandomForestClassifier(n_estimators = 100), X_train_count, y_train, X_test_count)\n",
    "print('RF, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:30.127233Z",
     "start_time": "2019-06-17T02:35:21.198110Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zXcTCTEGLtET",
    "outputId": "e4336045-c508-4372-e102-35c559f9a195"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('RF, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:40.162390Z",
     "start_time": "2019-06-17T02:35:30.607944Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EnvT8qvSLtEW",
    "outputId": "0c3a8fa0-7ee7-40ad-dc0e-85a8c3995733"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('RF, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:07.640904Z",
     "start_time": "2019-06-17T02:35:40.542371Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8jt-dTVELtEX",
    "outputId": "dc1113c9-b3f2-4d2e-f4d7-a5bcc98a020c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('RF, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:08.104108Z",
     "start_time": "2019-06-17T02:36:08.097127Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fVKeCH_VLtEZ"
   },
   "outputs": [],
   "source": [
    "results.loc['Random Forest'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyVz4Q6ILtEa"
   },
   "source": [
    "## Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:37.197296Z",
     "start_time": "2019-06-17T02:36:08.451184Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8wGvHTg-LtEb",
    "outputId": "dcc31f90-b0f8-4f5d-c835-25be80638e70"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Count Vectors\n",
    "accuracy1 = train_model(GradientBoostingClassifier(), X_train_count, y_train, X_test_count)\n",
    "print('GB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:52.852454Z",
     "start_time": "2019-06-17T02:36:37.714920Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HJNwQf57LtEd",
    "outputId": "686abbc5-8745-4dca-c2e2-b665b9d19901"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(GradientBoostingClassifier(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('GB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:37:02.608379Z",
     "start_time": "2019-06-17T02:36:53.252355Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iyPqLMgkLtEe",
    "outputId": "b7c87e04-f724-4ccf-e02e-28beb7e40654"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('GB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.314194Z",
     "start_time": "2019-06-17T02:37:03.039224Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1KYdyatTLtEg",
    "outputId": "aa4f303e-76ff-47e6-a04f-7e311e1848ad"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('GB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.683222Z",
     "start_time": "2019-06-17T02:39:14.675213Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "AC0hWO59LtEj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.loc['Gradient Boosting'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:15.024279Z",
     "start_time": "2019-06-17T02:39:15.010319Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "b9fY4J7XLtEk",
    "outputId": "ea4a8ddf-5b37-4cb9-da79-1920e1967b03"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_Pck1cuvLtDH",
    "mQCAUFWYLtDb",
    "OXwLriDpLtDq",
    "-2oNfajULtD4",
    "q1wYto68LtEE",
    "gLGxWK0yLtEO"
   ],
   "name": "IOD_Lab-9_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
